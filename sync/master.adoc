// tag::excludeDownstream[]
:org-name: AeroGear

:product-name: Mobile Services

:release-number: 1.0.0
:showcase-version: 0.8.0
:installer-release-number: 2.0.0
:xamarin-sdk-release-number: 2.0.1
:ios-sdk-release-number: 2.0.0
:android-sdk-release-number: 2.0.0

:service-name:

:mobile-client: Mobile App
:mobile-client-openshift: Mobile Client in your OpenShift project
:mobile-cli: Mobile CLI

// Metrics Service
:metrics-service: Mobile Metrics
:grafana-ui: Grafana
:prometheus-ui: Prometheus

// IDM Service
:keycloak-service: Identity Management
:keycloak-ui: Keycloak Admin UI
:keycloak-dashboard: Auth Dashboard
:idm-name: Keycloak
:idm-docs-link: https://www.keycloak.org/documentation.html

// Push Service

:unifiedpush-service: Push Notifications
:push-ui: Unified Push Admin UI
:push-notification: push notification

// Build Service
:mobile-ci-cd-service: Mobile CI/CD
:mobilecicd-ui: Jenkins UI

// Device Security
:device-security-service: Device Checks
:app-security-service: Mobile Security

// Sync Service
:sync-service: Data Sync
:sync-server: Voyager Server
:sync-client: Voyager Client

// TODO: fix this. There are versions on the backend framework libs
// And a version on the sync client (independent of each other also)
// :data-sync-version:

:SDK: AeroGear SDK
:ios-sdk: AeroGear SDK for iOS
:android-sdk: AeroGear SDK for Android
:js-sdk: AeroGear SDK for Cordova
:xamarin-sdk: AeroGear SDK for Xamarin

:mobile-developer-console: Mobile Developer Console

// links

:link-product-doc-home: /aerogear/latest
//:rn-link: {link-product-doc-home}/1.0_release_notes
:idm-guide-link: {link-product-doc-home}/identity-management.html
:metrics-guide-link: {link-product-doc-home}/mobile-metrics.html
:push-guide-link: {link-product-doc-home}/push-notifications.html
:device-security-guide-link: {link-product-doc-home}/device-security.html
:getting-started-guide-link: {link-product-doc-home}/getting-started.html

:data-sync-guide-link: {link-product-doc-home}/data-sync.html
:data-sync-queries-link: {link-product-doc-home}/ds-query.html
:data-sync-auth-link: {link-product-doc-home}/ds-auth.html
// end::excludeDownstream[]

:toc:
:toclevels: 4
:numbered:
:imagesdir: shared/images

:ProductName: Red Hat Managed Integration
:ProductShortName: Mobile Developer Services
:ProductRelease: 1.0
:ProductVersion: 1
:LatestVersion: 1.0

:DocInfoProductName: mobile_services
:DocInfoProductNumber: 1

:org-name: Red Hat
:product-name: Mobile Developer Services

:keycloak-ui: Red Hat SSO Admin UI
:idm-name: Red Hat Single Sign-On
:idm-docs-link:  https://access.redhat.com/products/red-hat-single-sign-on



:context: downstream

// ENVIRONMENT / CP SERVER - stage, qa, or production
// default env is prod
// ifndef::cp-server[]
:cp-server: access.redhat.com
// endif::[]

// BUILD LANGUAGE
// default language is en
// ifndef::language[]
:language: en-us
// endif::[]

// FORMAT AND PAGING
// default paging is single
:paging: html-single

// paged only for html format, all other formats (pdf, epub, single) use single
// ifeval::["{format}" == "html"]
:paging: html
// endif::[]

:link-product-doc-home: https://{cp-server}/documentation/{language}/{DocInfoProductName}
:rn-link: {link-product-doc-home}/{DocInfoProductNumber}/{paging}/1.0_release_notes
:idm-guide-link: {link-product-doc-home}/{DocInfoProductNumber}/{paging}/using_the_identity_management_mobile_service
:metrics-guide-link: {link-product-doc-home}/{DocInfoProductNumber}/{paging}/using_the_mobile_metrics_mobile_service
:push-guide-link: {link-product-doc-home}/{DocInfoProductNumber}/{paging}/using_the_push_notifications_mobile_service
:device-security-guide-link: {link-product-doc-home}/{DocInfoProductNumber}/{paging}/using_the_device_security_service
:getting-started-guide-link: {link-product-doc-home}/{DocInfoProductNumber}/{paging}/getting_started

:data-sync-guide-link: {link-product-doc-home}/{DocInfoProductNumber}/{paging}/developing_a_data_sync_app
:data-sync-queries-link: {link-product-doc-home}/{DocInfoProductNumber}/{paging}/developing_a_data_sync_app#querying_a_data_sync_server_using_a_data_sync_client
:data-sync-auth-link: {link-product-doc-home}/{DocInfoProductNumber}/{paging}/developing_a_data_sync_app#auth_data-sync

:context: data-sync
:parent-context: {context}
:service-name: {sync-service}

// :upstream-location: https://mobile-docs.netlify.com
// :upstream-location: ../../mobile-docs/build/site


[id='a-collection-of-modules']
= Developing a {sync-service} App

:leveloffset: +1




[[introduction]]

:leveloffset: 2

// The ID is used as an anchor for linking to the module. Avoid changing it after the module has been published to ensure existing links are not broken.
[id="introducing-data-sync-{context}"]

// The `context` attribute enables module reuse. Every module's ID includes {context}, which ensures that the module has a unique ID even if it is reused multiple times in a guide.
= Introducing Data Sync

Data Sync is a JavaScript framework that enables a developer to add real time data synchronization to both mobile and web clients.
The Data Sync framework also provides offline capabilities that allow a client to continue operating offline and once connectivity is re-established, the client is automatically synchronized.
An app built using the Data Sync framework typically connects to a data source for data persistence however, an app built using the Data Sync framework works without a data source.

An app built using the Data Sync framework comprises of two components:

* The Data Sync client is a JavaScript client offering client side extensions and server side integration.
The Data Sync client can be integrated into frameworks such as React and Angular.

* The Data Sync server is a framework for building Node.js based GraphQL API.
The Data Sync server offers enterprise extensions for ensuring data security, integrity, and monitoring.
It can be integrated into existing Node.js application.

The {sync-service} framework uses the link:https://www.apollographql.com/[Apollo platform] as the GraphQL implementation.

.Additional resources

* Real-time data synchronization across mobile and web clients.
** Websockets allow for real-time data synchronization across multiple Data Sync clients. Data Sync clients receive updates from the Data Sync server without having to explicitly query their local data as conflict detection is handled by the Data Sync server.

* A Data Sync client can perform any operation regardless of the connectivity state.
** If network connectivity is a concern, a Data Sync client can perform any operation regardless of its connectivity state. A Data Sync client can perform the same operations when it is on-line or off-line, and this functionality ensures that you can safely use Data Sync to create business critical applications.

* Offers fully customizable conflict detection and resolution to the developer.
** Data Sync enables users to detect and resolve conflicts on the Data Sync server resulting in the seamless transmission of data to various Data Sync clients. Data Sync also allows for conflict resolution on the Data Sync client should a developer want to adopt this strategy.

* Instant synchronous queries provide instant feedback for developers.
** When a Data Sync client is on-line, instant queries allow a developer to quickly react to errors and display results to users when the operation is executed. Developers can retrieve an instant response or error from the Data Sync server however the Data Sync client must have a connection to the Data Sync server.

* Flexible data sources.
** Data Sync can connect to various data sources, for example, cloud storage, databases such as MongoDB and PostgreSQL, and existing back-end data sources.

:leveloffset: 2

:leveloffset: 2

[id="data-sync-technical-overview-{context}"]
= Data Sync Technical Overview

This section describes the technical aspects of Data Sync.



image::data-sync-technical-overview.png[Data Sync Technical Overview]


.Data Sync case study
[options="header"]
|====
|Component|Technical Role
|Sync Client| The Sync Client is a client side JavaScript library used for building web and mobile applications. It allows for simple Sync Server integration.
|Sync Server| The Sync Server is based on the Apollo Server framework and it performs two primary functions. It sends and retrieves data from a data source, and it syncs data across the Sync Clients. Sync Server uses GraphQL to create custom connections that in turn allow various types of Sync Clients to connect.
|Data sources| The data source stores data. This data is typically what is synchronized across the Sync Clients.
|====

For more information about the Apollo Server framework, see link:https://www.apollographql.com/docs/apollo-server/[_Start here to learn about the Apollo platform_].

:leveloffset: 2

:leveloffset: +2

//':context:' is a vital parameter. See: http://asciidoctor.org/docs/user-manual/#include-multiple
[id='ref-data-sync-terminology-{context}']
= Data Sync terminology

This section describes terminology that is associated with Data Sync.

.Data Sync terms

GraphQL:: A query language for your API, and a server-side runtime for executing queries that use a type system. For more information, see link:https://graphql.org/learn[GraphQL].

Apollo:: link:https://www.apollographql.com/[Apollo] is an implementation of GraphQL designed for the needs of product engineering teams building modern, data-driven applications.
Apollo includes two open-source libraries, Apollo Server and Apollo Client.
The Data Sync Framework leverages Apollo functionality.

Sync Server:: The Sync Server is a framework for building Node.js based GraphQL API.

Sync Client:: The Sync Client is a JavaScript client offering client side extensions and server side integration. The Sync Client can be integrated into frameworks such as React and Angular.

Data sources:: Data Sync framework is typically used in conjunction with a data source for data persistence however, an app built using the Data Sync framework works without a data source.

Data Sync framework:: Data Sync is a JavaScript framework that enables a developer to add the capability to synchronize data in real-time for both mobile and web clients.

:leveloffset: 2

[id='{context}_additional-resources-{context}']

.Additional resources

* link:https://graphql.org/learn[Learn GraphQL]
* link:https://github.com/aerogear/voyager-server[Voyager Server GitHub repository]
* link:https://github.com/aerogear/aerogear-js-sdk/tree/master/packages/sync[Voyager Client GitHub repository]
* link:https://www.apollographql.com/docs/apollo-server[Apollo Server]
* link:https://www.apollographql.com/docs/react[Apollo Client]

:service-name: {sync-service}

:leveloffset!:

:leveloffset: +1


= Developing a {sync-service} Application
:page-partial:

Unlike other mobile services which provide a server and an API, Data Sync is a framework that you use to develop services. Typically, you develop a {sync-service} service as follows:

. Design a link:https://graphql.org/learn[GraphQL] schema.
. Develop a Data Sync server and Data Sync client, with the features you require, for example, real-time updates.
. Containerize your Data Sync server and deploy it to OpenShift.
. Bind your mobile app to that Data Sync server.
. Configure your mobile app to point to the Data Sync server.
. Complete your mobile app development.
. Build and run your mobile app.

:leveloffset!:



:leveloffset: +1


:leveloffset: 1

[id="getting-started-with-hello-world-{context}"]
= Getting Started with Hello World {sync-service}

In this example, you add the {sync-server} library to your link:https://expressjs.com/[Express] node.js project, create an `index-1.js` file, run the server, and query GraphQL.

* {sync-server} is a set of Node.js libraries that can be used to build a Data Sync server.

* {sync-server} is the starting point for developing a {sync-service} application.


.Prerequisites

* You have Node.js and npm installed.
* You have created a node.js project.

.Procedure

. Add libraries to your Node.js application:
+
====
[source,bash]
----
$ npm install graphql <1>
$ npm install express <2>
$ npm install @aerogear/voyager-server <3>
----

<1> See https://graphql.org/
<2> See https://expressjs.com/
<3> The {sync-server} library that enables data sync
====

. Create an `index-1.js` file with the following content:
+
[source,javascript]
----
const express = require('express')
//Include our server libraries
const { VoyagerServer, gql } = require('@aerogear/voyager-server')

//Provide your graphql schema
const typeDefs = gql`
  type Query {
    hello: String
  }
`

//Create the resolvers for your schema
const resolvers = {
  Query: {
    hello: (obj, args, context, info) => {
      return `Hello world`
    }
  }
}

//Initialize the library with your Graphql information
const apolloServer = VoyagerServer({
  typeDefs,
  resolvers
})

//Connect the server to express
const app = express()
apolloServer.applyMiddleware({ app })

app.listen(4000, () =>
  console.log(`🚀 Server ready at http://localhost:4000/graphql`)
)
----

. Run the server:
+
[source,bash]
----
$ node index-1.js

🚀 Server ready at http://localhost:4000/graphql
----

. Browse `http://localhost:4000/graphql` and interact with the playground. For example:
+
[source,javascript]
----
{
  hello
}
----

. Check the output. For the example above, the output should be:
+
[source,javascript]
----
{
  "data": {
    "hello": "Hello world"
  }
}
----

To get started with the  {sync-service} framework, see the link:https://github.com/aerogear/ionic-showcase[sample application].
In this app, you can explore a more complex schema.

Before proceeding, make sure you have an understanding of the following GraphQL concepts:

* Schema design
* Resolvers
* Subscriptions

:leveloffset: 1

:leveloffset!:
:leveloffset: +1


= Querying a Data Sync server using a Data Sync client

.Prerequisites
* You have Node.js and npm installed.
* You have created an empty web project that supports ES6, for example, using the link:https://webpack.js.org/guides/getting-started/[webpack getting started] guide.
* You have completed the server getting started guide and the application is running.

This section describes how to use the {sync-client} to create mobile and web applications that can communicate with the Voyager server application.

Data Sync provides JavaScript libraries which integrate your javascript app using with a server that also uses Data Sync.
The client libraries are based on the link:https://www.apollographql.com/docs/react/api/apollo-client.html[Apollo client].

You will add the libraries to your mobile project, configure the client classes, connect to the server, and confirm that it works.

. Create an address book server:
.. Create an `index-2.js` file with the following content:
+
[source,javascript]
----
const express = require('express')
//Include our server libraries
const { VoyagerServer, gql } = require('@aerogear/voyager-server')

//Provide your graphql schema
const typeDefs = gql`
type Query {
  info: String!
  addressBook: [Person!]!
}

type Mutation {
  post(name: String!, address: String!): Person!
}

type Person {
  id: ID!
  address: String!
  name: String!
}
`

let persons = [{
  id: 'person-0',
  name: 'Alice Roberts',
  address: '1 Red Square, Waterford'
}]

let idCount = persons.length
const resolvers = {
  Query: {
    info: () => `This is a simple example`,
    addressBook: () => persons,
  },
  Mutation: {

    post: (parent, args) => {
       const person = {
        id: `person-${idCount++}`,
        address: args.address,
        name: args.name,
      }
      persons.push(person)
      return person
    }
  },
}

//Initialize the library with your Graphql information
const apolloServer = VoyagerServer({
  typeDefs,
  resolvers
})

//Connect the server to express
const app = express()
apolloServer.applyMiddleware({ app })

app.listen(4000, () =>
  console.log(`🚀 Server ready at http://localhost:4000/graphql`)
)

----

.. Run the server:
+
[source,bash]
----
$ node index-2.js

🚀 Server ready at http://localhost:4000/graphql
----

.. Browse `http://localhost:4000/graphql` and interact with the playground. For example:
+
[source,javascript]
----
{
  addressBook {
    name
    address

  }
}
----

.. Check the output. For the example above, the output should be:
+
[source,javascript]
----
{
  "data": {
    "addressBook": [
      {
        "name": "Alice Roberts",
        "address": "1 Red Square, Waterford"
      }
    ]
  }
}
----




. Add the following libraries to your javascript client:
+
[source,bash]
----
npm install @aerogear/voyager-client
npm install graphql
npm install graphql-tag
----
+
NOTE: A prerequisite is that you have created an empty web project that supports ES6, for example, using the link:https://webpack.js.org/guides/getting-started/[webpack getting started] guide.

. Create an `index.js` file to make the same query as step 1, but from JavaScript.
+
In this example, a config object is created, and the `httpUrl` field is set to the url of the Voyager server application.
If the client app uses subscriptions, then the `wsUrl` field is required too.
+
.src/index.js
[source,javascript]
----
// gql is a utility function that handles gql queries
import gql from 'graphql-tag';

import { OfflineClient } from '@aerogear/voyager-client';

// connect to the local service.
let config = {
  httpUrl: "http://localhost:4000/graphql",
  wsUrl: "ws://localhost:4000/graphql",
}

async function queryPeople() {

  // Actually create the client
  let offlineClient = new OfflineClient(config);
  let client = await offlineClient.init();

  // Execute the query
  client.query({
      fetchPolicy: 'network-only',
      query: gql`
      query addressBook{
        addressBook{
        name
        address
        }
      }
      `
    })
    //Print the response of the query
    .then( ({data}) => {
      console.log(data.addressBook)
    });
}

queryPeople();
----

. Build and run the client application.
. Browse the client application and check the console output.
+
It should include an array similar to the following:
+
----
address: "1 Red Square, Waterford"
name: "Alice Roberts"
__typename: "Person"
----

:leveloffset!:
:leveloffset: +1



= Adding a mutation to a Data Sync client

.Prerequisites
* You have Node.js and npm installed.
* You have completed link:{data-sync-queries-link}[Queries section] and the server is still running.


. Modify the client application to perform the mutation:
+
.src/index.js
[source,javascript]
----
// gql is a utility function that handles gql queries
import gql from 'graphql-tag';

import { OfflineClient } from '@aerogear/voyager-client';

// connect to the local service.
let config = {
  httpUrl: "http://localhost:4000/graphql",
  wsUrl: "ws://localhost:4000/graphql",
}

async function addPerson() {

  // Actually create the client
  let offlineClient = new OfflineClient(config);
  let client = await offlineClient.init();

  // Execute the mutation
  client.mutate({
      mutation: gql`
       mutation {
         post(name: "John Doe", address: "1 Red Hill") {
           id
         }
       }
       `
    })
    //Print the response of the query
    .then( ({data}) => {
      console.log(data)
    });
}

addPerson();
----

. Build and run the client application.
. Browse the client application and check the console output.
+
It should include an array similar to the following:
+
----
{
  "data": {
    "post": {
      "id": "person-1"
    }
  }
}
----

:leveloffset!:

:leveloffset: +1



[[offline-client]]
= Supporting offline functionality in your mobile app
:toc:

== About offline functionality

Your mobile app can run offline and allow users to query and create mutations using the @aerogear/voyager-client module.

As shown in the diagram below, all queries are performed against the cache, a mutation store supports offline mutations.

image::datasync-features.png[]

NOTE: The mutation store is sometimes referred to as the offline store.

If a client goes offline for a long period of time, the mutation store negotiates local updates with the server using conflict resolution strategies.

When a client comes online again, the mutations are replicated back to the server, as shown in the diagram below:

image::datasync-going_offline.png[]

Developers can attach listeners to get notifications about updates applied on the server or failing, and take appropriate actions.

.Mutations and Local Cache

By default queries and the results of mutations are cached.

Mutations can change query results, make sure to call the `refetchQueries` or `update` options of the `mutate` method to ensure the local cache is kept up to date.

The @aerogear/voyager-client module also provides cache helper functions to reduce the amount of code required, as described in xref:cache-update-helpers[].

For more information about `mutate` and the options available, see link:https://www.apollographql.com/docs/react/essentials/mutations.html#props[Apollo's document about mutations].


[#setup-offline-client]
== Creating an offline client


The @aerogear/voyager-client module provides an `OfflineClient` class which exposes the following functionality:

* direct access to the mutation store
* allows you register multiple offline event listeners as described in xref:sync-client-offline-queue-listener[]
* automatically ensures the mobile app's local cache is kept up to date. This client automatically generates `update` methods as described in xref:cache-update-helpers[].

To create the client:

[source,javascript]
----
import { OfflineClient } from '@aerogear/voyager-client';

let config = {
  httpUrl: "http://localhost:4000/graphql",
  wsUrl: "ws://localhost:4000/graphql",
}

async function setupClient() {

  let offlineClient = new OfflineClient(config);
  let client = await offlineClient.init();
}

setupClient();
----

This client can replace an Apollo client as it supports the same functionality.



= Detecting mutations while offline

If a mutation occurs while the device is offline, the `client.mutate` function:

- returns immediately
- returns a promise with an error

You can check the _error_ object to isolate errors relating to offline state.
Invoking the `watchOfflineChange()` method on an _error_ object watches for when an offline change is synced with the server, and sends a notification when triggered.

For example:
[source, javascript]
----
  client.mutate(...).catch((error)=> {
    // 1. Detect if this was an offline error
   if(error.networkError && error.networkError.offline){
     const offlineError: OfflineError =  error.networkError;
     // 2. We can still track when offline change is going to be replicated.
     offlineError.watchOfflineChange().then(...)
   }
  });
----

NOTE: In addition to watching individual mutations, you can add a global offline listener when creating a client as described in xref:sync-client-offline-queue-listener[].

= Performing mutations while offline

The @aerogear/voyager-client module provides an `offlineMutate` method which extends Apollo's mutate function with some extra functionality.
This includes automatically adding some fields to each operation's context.

To set up the offline client, see xref:setup-offline-client[].

Once set up is complete, `offlineMutate` is then available to use.

Note: The `offlineMutate` method accepts the same parameters as `mutate` with some additional optional parameters also available.


[source,javascript]
----
  const { CacheOperation } = require('@aerogear/voyager-client');

  client.offlineMutate({
    ...
    updateQuery: GET_TASKS, // <1>
    operationType: CacheOperation.ADD, // <2>
    idField: "id", // <3>
    returnType: "Task" // <4>
    ...
  })
----
<1> The query or queries which should be updated with the result of the mutation.
<2> The type of operation being performed. Should be "add", "refresh" or "delete". Defaults to "add" if not provided.
<3> The field on the object used to identify it. Defaults to "id" if not provided.
<4> The type of object being operated on.

== Supporting app restarts while offline

An Apollo client holds all mutation parameters in memory.
An offline Apollo client continues to store mutation parameters and once online, it restores all mutations to memory.
Any update functions that are supplied to mutations cannot be cached by an Apollo client resulting in the loss of all optimistic responses after a restart.
_Update functions_ supplied to mutations cannot be saved in the cache.
As a result, all _optimisticResponses_ disappear from the application after a restart and  only reappear when the Apollo client becomes online and successfully syncs with the server.

To prevent the loss of all _optimisticResponses_ after a restart, you can configure the _Update Functions_ to restore all _optimisticResponses_.

[source, javascript]
----
const updateFunctions = {
  // Can contain update functions from each component
  ...ItemUpdates,
  ...TasksUpdates
}

let config = {
  mutationCacheUpdates: updateFunctions,
}
----

You can also use `getUpdateFunction` to automatically generate functions:

[source, javascript]
----
const { createMutationOptions, CacheOperation } = require('@aerogear/voyager-client');

const updateFunctions = {
  // Can contain update functions from each component
  createTask: getUpdateFunction({
      mutationName: 'createTask',
      idField: 'id',
      updateQuery: GET_TASKS,
      operationType: CacheOperation.ADD
    }),
  deleteTask: getUpdateFunction({
      mutationName: 'deleteTask',
      idField: 'id',
      updateQuery: GET_TASKS,
      operationType: CacheOperation.DELETE
    })
}

let config = {
  ...
  mutationCacheUpdates: updateFunctions,
  ...
}
----

== Ensuring specified mutations are performed online only

If you wish to ensure certain mutations are only executed when the client is online, use the GraphQL directive `@onlineOnly`, for example:

[source, graphql]
----
exampleMutation(...) @onlineOnly {
  ...
}
----

[#sync-client-offline-queue-listener]
== Listening for events

To handle all notifications about offline related events, use the *offlineQueueListener* listener in the config object

The following events are emitted:

* `onOperationEnqueued` - Called when new operation is being added to offline queue
* `onOperationSuccess` - Called when back online and operation succeeds
* `onOperationFailure` - Called when back online and operation fails with GraphQL error
* `queueCleared` - Called when offline operation queue is cleared

You can use this listener to build User Interfaces that show pending changes.

[[cache-update-helpers]]
== Using cache update helpers

The @aerogear/voyager-client module provides an out of the box solution for managing updates to your application's cache.
It can intelligently generate cache update methods for both mutations and subscriptions.

=== Using cache update helpers for mutations

The following example shows how to use these helper methods for mutations.
To use these methods, create an offline client as described in xref:setup-offline-client[] and then use the  `offlineMutate` method.
The `offlineMutate` function accepts a `MutationHelperOptions` object as a parameter.

[source, javascript]
----
const { createMutationOptions, CacheOperation } = require('@aerogear/voyager-client');

const mutationOptions = {
  mutation: ADD_TASK,
  variables: {
    title: 'item title'
  },
  updateQuery: {
    query: GET_TASKS,
    variables: {
      filterBy: 'some filter'
    }
  },
  typeName: 'Task',
  operationType: CacheOperation.ADD,
  idField: 'id'
};
----

You can also provide more than one query to update the cache by providing an array to the `updateQuery` parameter:

[source, javascript]
----

const mutationOptions = {
  ...
  updateQuery: [
    { query: GET_TASKS, variables: {} }
  ]
  ,
  ...
};
----

The following example shows how to prepare an offline mutation to add a task using the `mutationOptions` object and how to update the `GET_TASK` query for the client's cache.

[source, javascript]
----
const { createMutationOptions, CacheOperation } = require('@aerogear/voyager-client');

client.offlineMutate<Task>(mutationOptions);
----

If you do not want to use the offline client you can also use the `createMutationOptions` function directly.
This function provides an Apollo compatible `MutationOptions` object to pass to your pre-existing client.
The following example shows how to use this function where `mutationOptions` is the same object as the previous code example.

[source, javascript]
----
const options = createMutationOptions(mutationOptions);

client.mutate<Task>(options);
----

=== Using cache update helpers for subscriptions

The @aerogear/voyager-client module provides a subscription helper which can generate the necessary options to be used with Apollo Client's `subscribeToMore` function.

To use this helper, we first need to create some options, for example:

[source, javascript]
----
const { CacheOperation } = require('@aerogear/voyager-client');

const options = {
  subscriptionQuery: TASK_ADDED_SUBSCRIPTION,
  cacheUpdateQuery: GET_TASKS,
  operationType: CacheOperation.ADD
}
----

This options object informs the subscription helper that for every data object
received because of the `TASK_ADDED_SUBSCRIPTION` the `GET_TASKS` query should also be kept up to date in the cache.

You can then create the required cache update functions:

[source, javascript]
----
const { createSubscriptionOptions } = require('@aerogear/voyager-client');

const subscriptionOptions = createSubscriptionOptions(options);
----

To use this helper, pass this `subscriptionOptions` variable to the `subscribeToMore` function of our `ObservableQuery`.

[source, javascript]
----

const query = client.watchQuery<AllTasks>({
  query: GET_TASKS
});

query.subscribeToMore(subscriptionOptions);
----

The cache is kept up to date while automatically performing data deduplication.

=== Using cache update helpers for multiple subscriptions

The @aerogear/voyager-client module provides the ability to automatically call `subscribeToMore` on your `ObservableQuery`.
This can be useful in a situation where you may have multiple subscriptions which can affect one single query.
For example, if you have a `TaskAdded`, `TaskDeleted`, and a `TaskUpdated` subscription you require three separate `subscribeToMore` function calls.
To avoid this, use the `subscribeToMoreHelper` function from the @aerogear/voyager-client module to automatically handle this by passing an array of subscriptions and their corresponding queries:

[source, javascript]
----
const { CacheOperation } = require('@aerogear/voyager-client');

const addOptions = {
  subscriptionQuery: TASK_ADDED_SUBSCRIPTION,
  cacheUpdateQuery: GET_TASKS,
  operationType: CacheOperation.ADD
}

const deleteOptions = {
  subscriptionQuery: TASK_DELETED_SUBSCRIPTION,
  cacheUpdateQuery: GET_TASKS,
  operationType: CacheOperation.DELETE
}

const updateOptions = {
  subscriptionQuery: TASK_UPDATED_SUBSCRIPTION,
  cacheUpdateQuery: GET_TASKS,
  operationType: CacheOperation.REFRESH
}

const query = client.watchQuery<AllTasks>({
  query: GET_TASKS
});

subscribeToMoreHelper(query, [addOptions, deleteOptions, updateOptions]);
----

= Detecting Network Status

Use the NetworkStatus interface to check the current network status, or to register a listener which performs actions when the status of the network changes.

Two default implementations are provided:

* *WebNetworkStatus*  for web browsers
* *CordovaNetworkStatus* for Cordova

The following example demonstrates how to register a listener using `CordovaNetworkStatus`:

[source, javascript]
----

import { CordovaNetworkStatus, NetworkInfo } from '@aerogear/voyager-client';
const networkStatus = new CordovaNetworkStatus();

networkStatus.onStatusChangeListener({
  onStatusChange: info => {
    const online = info.online;
    if (online) {
      //client is online, perform some actions
    } else {
      //client is offline
    }
  }
});

let config = {
  ...
  networkStatus: networkStatus,
  ...
};

//create a new client using the config
----

:leveloffset!:

:leveloffset: +1


[id='realtime_{context}']
= Supporting real-time updates in your mobile app
:page-partial:
:toc:


:leveloffset: 1

[id="realtime-intro-{context}"]
= Introduction to real-time updates

After developing some queries and mutations, you might want to implement real-time updates.
These are supported in the GraphQL specification by an operation type called `Subscription`.
To support subscriptions in a production environment, Data Sync implements subscriptions using an MQTT PubSub subscription mechanism, however you might want to use the Apollo PubSub module to develop proof-of-concept applications.

When coding for real-time updates, you use the following modules:

* @aerogear/voyager-server - supports clients that use voyager-client to enable GraphQL queries and mutations
* @aerogear/voyager-subscriptions - supports clients that use voyager-client to enable GraphQL subscriptions
* @aerogear/graphql-mqtt-subscriptions - supports GraphQL resolvers connections to a MQTT broker

GraphQL Subscriptions enable clients to subscribe to server events over a websocket connection.

The flow can be summarized as follows:

* Client connects to the server using websockets, and subscribes to certain events.
* As events occur, the server notifies the clients that are subscribed to those events.
* Any _currently connected_ client that is subscribed to a given event receives it.
* The client can close the connection at any time and no longer receives updates.

To receive updates, the client must be currently connected to the server.
The client does not receive events from subscriptions while offline.
To support inactive clients, use Push Notifications.

.Additional resources

* For more information about GraphQL subscriptions, see the link:https://www.apollographql.com/docs/apollo-server/features/subscriptions.html[Subscriptions documentation].

:leveloffset!:

:leveloffset: 1

[id="realtime-updates-{context}"]
= Implementing real-time updates on a Data Sync server

The follow code shows typical code for a Data Sync Server without subscriptions:

[%collapsible]
====
[source,js]
----
const apolloServer = VoyagerServer({
  typeDefs,
  resolvers
})

const app = express()
apolloServer.applyMiddleware({ app })

app.listen({ port }, () =>
  console.log(`🚀 Server ready at http://localhost:${port}${apolloServer.graphqlPath}`)
)
----
====

The following sections outline the steps required to enable real-time updates:

. Implement a SubscriptionServer
. Implement a Publish Subscribe Mechanism
. Define subscriptions in the schema
. Implement resolvers

== Implementing a SubscriptionServer using voyager-subscription

To allow you create GraphQL subscription types in your schema:

. Install the `@aerogear/voyager-subscriptions` package:
+
----
$ npm i @aerogear/voyager-subscriptions
----

. Configure SubscriptionServer using `@aerogear/voyager-subscriptions`
+
[%collapsible]
====
[source,js]
----
const { createSubscriptionServer } = require('@aerogear/voyager-subscriptions')

const apolloServer = VoyagerServer({
  typeDefs,
  resolvers
})

const app = express()
apolloServer.applyMiddleware({ app })
const port = 4000

const server = app.listen({ port }, () => {
  console.log(`🚀 Server ready at http://localhost:${port}${apolloServer.graphqlPath}`)

  createSubscriptionServer({ schema: apolloServer.schema }, {
    server,
    path: '/graphql'
  })
})
----
====
+
The `createSubscriptionServer` code:
+
* returns a `SubscriptionServer` instance
* installs handlers for
** managing websocket connections
** delivering subscriptions on the server
* provides integrations with other modules such as `@aerogear/voyager-keycloak`.

.Additional Information

For more information about arguments and options, see the https://npm.im/subscriptions-transport-ws[subscriptions-transport-ws] module.


== Implementing a Publish Subscribe Mechanism

WARNING: This procedure describes an in-memory implementation which is useful for prototyping but not suitable for production. {org-name} recommends using link:npm.im/@aerogear/graphql-mqtt-subscriptions[MQTT PubSub] in production. See xref:pub-sub[] for more information about all the implementation methods.

To provide a channel to push updates to the client using the default `PubSub` provided by `apollo-server`, you implement a Publish Subscribe mechanism, for example:

[%collapsible]
====
[source,js]
----
const { PubSub } = require('apollo-server')

const pubsub = new PubSub()
----
====

.Addtional Information
Subscriptions depend on a https://en.wikipedia.org/wiki/Publish%E2%80%93subscribe_pattern[publish subscribe] mechanism to generate the events that notify a subscription. There are https://www.apollographql.com/docs/apollo-server/features/subscriptions/#pubsub-implementations[several PubSub implementations] available based on the `PubSubEngine` interface.


== Defining subscriptions in the schema

Subscriptions are a root level type.
They are defined in the schema similar to `Query` and `Mutation`.
For example, in the following schema, a `Task` type is defined and so are mutations and subscriptions.

[%collapsible]
====
----
type Subscription {
  taskCreated: Task
}

type Mutation {
  createTask(title: String!, description: String!): Task
}

type Task {
  id: ID!
  title: String!
  description: String!
}
----
====


== Implementing resolvers

Inside the resolver map, subscription resolvers return an `AsyncIterator,` which listens for events.
To generate an event, call the `publish` method.
The `pubsub.publish` code is typically located inside a mutation resolver.

In the following example, when a new task is created, the `createTask` resolver publishes the result of this mutation to the `TaskCreated` channel.

[%collapsible]
====
[source,js]
----
const TASK_CREATED = 'TaskCreated'

const resolvers = {
  Subscription: {
    taskCreated: {
      subscribe: () => pubSub.asyncIterator(TASK_CREATED)
    }
  },
  Mutation: {
    createTask: async (obj, args, context, info) => {
      const task = tasks.create(args)
      pubSub.publish(TASK_CREATED, { taskCreated: task })
      return task
    }
  },
}
----
====

NOTE: This subscription server does not implement authentication or authorization. For information about implementing authenication and authorization, see link:{data-sync-auth-link}[Supporting authentication and authorization in your mobile app].

.Additional Information

For information on how to use subscriptions in your client code, see xref:sync-js-client-realtime-updates[Realtime Updates].



:leveloffset!:

:leveloffset: 1

[id="pub-sub"]
= Configuring a Publish Subscribe mechanism

You can use the Apollo PubSub mechanism for development, but you must use the MQTT PubSub mechanism for production.

== Using the Apollo PubSub mechanism

The xref:realtime-updates-{context}[] section describes how to set up the default `PubSub` provided by `apollo-server`. For a production system, you require link:npm.im/@aerogear/graphql-mqtt-subscriptions[MQTT PubSub].


== Using the MQTT PubSub mechanism

// TODO: check link
The https://npm.im/@aerogear/graphql-mqtt-subscriptions[`@aerogear/graphql-mqtt-subscriptions`] module provides an `AsyncIterator` interface used for xref:realtime-updates-{context}[implementing subscription resolvers]
It connects the {sync-service} server to an MQTT broker to support horizontally scalable subscriptions.



Initialize an MQTT client and pass that client to the `@aerogeaar/graphql-mqtt-subscriptions` module, for example:

[%collapsible]
====
[source,js]
----
const mqtt = require('mqtt')
const { MQTTPubSub } = require('@aerogear/graphql-mqtt-subscriptions')

const client = mqtt.connect('mqtt://test.mosquitto.org', {
  reconnectPeriod: 1000,
})

const pubsub = new MQTTPubSub({
  client
})
----
====

In the example, an `mqtt` client is created using `mqtt.connect` and then this client is passed into an `MQTTPubSub` instance.
The `pubsub` instance can then be used to publish and subscribe to events in the server.

// TODO maybe link or show example code in showcase app?

.Additional Information

* link:https://www.npmjs.com/package/mqtt#connect[mqtt.connect documentation].
* link:https://npmjs.com/package/@aerogear/graphql-mqtt-subscriptions[MQTTPubSub documentation]

:leveloffset!:

:leveloffset: 2

= Configuring AMQ Online for MQTT Messaging

Red Hat AMQ supports the MQTT protocol which makes it a suitable PubSub technology for powering GraphQL subscriptions at scale.

This section provides recommendations for

* Configuring AMQ Online for MQTT messaging.
* Connecting to AMQ Online and using it as a pubsub within server applications.

.Terminology

* https://access.redhat.com/documentation/en-us/red_hat_amq/7.2/html-single/using_amq_online_on_openshift_container_platform/index#assembly-intro-using-messaging[AMQ Online] is a mechanism that allows developers to consume the features of Red Hat AMQ within OpenShift.

* https://access.redhat.com/documentation/en-us/red_hat_amq/7.3/html/introducing_red_hat_amq_7/about[Red Hat AMQ] provides fast, lightweight, and secure messaging for Internet-scale applications. AMQ Broker supports multiple protocols and fast message persistence.

* http://mqtt.org/[MQTT] stands for MQ Telemetry Transport. It is a publish/subscribe, extremely simple and lightweight messaging protocol.


AMQ Online includes many configuration options that could address the specific needs of your application.
The minimum configuration steps for using AMQ Online for MQTT messaging and enabling GraphQL subscriptions are:

. Create an `AddressSpace`
. Create an `Address`
. Create a `MessagingUser`

== Creating an address space

A user can request messaging resources by creating an `AddressSpace`. There are two types of address space, `standard` and `brokered`.
You must use the `brokered` address space for MQTT based applications.

.Procedure
. Create an address space, for example, the following resource creates a brokered `AddressSpace`:
+
[source,yaml,options="nowrap"]
----
apiVersion: enmasse.io/v1beta1
kind: AddressSpace
metadata:
  name: myaddressspace
spec:
  type: brokered
  plan: brokered-single-broker
----

. Create the `AddressSpace`.
+
----
oc create -f brokered-address-space.yaml
----

. Check the status of the address space:
+
----
oc get <`AddressSpace` name> -o yaml
----
+
The output displays information about the address space, including details required for connecting applications.

.Additional Information

See https://access.redhat.com/documentation/en-us/red_hat_amq/7.3/html-single/using_amq_online_on_openshift_container_platform/index#create-address-space-cli-messaging[Creating address spaces using the command line] for more information.

== Creating an Address

An adress is part of an `AddressSpace` and represents a destination for sending and receiving messages.
Use an `Address` with type `topic` to represent an MQTT topic.

. Create an address definition:
+
----
apiVersion: enmasse.io/v1beta1
kind: Address
metadata:
    name: myaddressspace.myaddress # must have the format <`AddressSpace` name>.<address name>
spec:
    address: myaddress
    type: topic
    plan: brokered-topic
----

. Create the address:
+
----
oc create -f topic-address.yaml
----

NOTE: See the xref:#realtime-updates-{context}[Configuring your server for real-time updates] guide for more information about using `pubsub.asyncIterator()`.
Create an Address for each topic name passed into `pubsub.asyncIterator()`.

.Additional Information

See https://access.redhat.com/documentation/en-us/red_hat_amq/7.2/html-single/using_amq_online_on_openshift_container_platform/index#create-address-cli-messaging[Creating addresses using the command line] for more information.

== Creating an AMQ Online user

A messaging client connects using an AMQ Online user, also known as a`MessagingUser`.
A `MessagingUser` specifies an authorization policy that controls which addresses can be used and the operations that can be performed on those addresses.

Users are configured as `MessagingUser` resources.
Users can be created, deleted, read, updated, and listed.

. Create a user definition:
+
----
apiVersion: user.enmasse.io/v1beta1
kind: MessagingUser
metadata:
  name: myaddressspace.mymessaginguser # must be in the format <`AddressSpace` name>.<username>
spec:
  username: mymessaginguser
  authentication:
    type: password
    password: cGFzc3dvcmQ= # must be Base64 encoded. Password is 'password'
  authorization:
    - addresses: ["*"]
      operations: ["send", "recv"]
----

. Create the `MessagingUser`.
+
----
oc create -f my-messaging-user.yaml
----

An application can now connect to an AMQ Online address using this user's credentials.

For more information see the https://access.redhat.com/documentation/en-us/red_hat_amq/7.2/html-single/using_amq_online_on_openshift_container_platform/index#con-user-model-messaging[AMQ Online User Model].

= Using GraphQL MQTT PubSub with AMQ Online

.Prerequisites


The following AMQ Online resources are available for MQTT Applications

* AddressSpace
* Address
* MessagingUser

This section describes how to use https://npm.im/@aerogear/graphql-mqtt-subscriptions[`@aerogear/graphql-mqtt-subscriptions`] to connect to an AMQ Online `Address`.

. Retrieve the connection details for the `AddressSpace` you want to use:
+
----
oc get addressspace <addressspace> -o yaml
----

. Determine which method you want to use to connect to the address:
+
* Using the service hostname - Allows clients to connect from within the OpenShift cluster.
+
{org-name} recommends that applications running inside OpenShift connect using the service hostname.
The service hostname is only accessible within the OpenShift cluster. This ensures messages routed between your application and AMQ Online stay within the OpenShift cluster and never go onto the public internet.
+
* Using the external hostname - Allows clients to connect from outside the OpenShift cluster.
+
The external hostname allows connections from outside the OpenShift cluster. This is useful for the following cases:
+
** Production applications running outside of OpenShift connecting and publishing messages.
** Quick Prototyping and local development. Create a non-production `AddressSpace`, allowing developers to connect applications from their local environments.



. To connect to an AMQ Online `Address` using the service hostname
.. Retrieve the service hostname:
+
[source,bash]
----
oc get addressspace <addressspace name> -o jsonpath='{.status.endpointStatuses[?(@.name=="messaging")].serviceHost
----
.. Add code to create the connection, for example:
+
[source,js]
----
const mqtt = require('mqtt')
const { MQTTPubSub } = require('@aerogear/graphql-mqtt-subscriptions')

const client = mqtt.connect({
  host: '<internal host name>',
  username: '<MessagingUser name>',
  password: '<MessagingUser password>',
  port: 5762,
})

const pubsub = new MQTTPubSub({ client })
----

.. To encrypt all messages between your application and the AMQ Online broker, enable TLS, for example:
+
[source,js]
----
const mqtt = require('mqtt')
const { MQTTPubSub } = require('@aerogear/graphql-mqtt-subscriptions')

const host = '<internal host name>'

const client = mqtt.connect({
  host: host,
  servername: host,
  username: '<MessagingUser name>',
  password: '<MessagingUser password>',
  port: 5761,
  protocol: 'tls',
  rejectUnauthorized: false,
})

const pubsub = new MQTTPubSub({ client })
----

. To connect to an AMQ Online `Address` using the external hostname:
+
NOTE: The external hostname typically accept only accept TLS connections.

.. Retrieve the external hostname:
+
[source,bash]
----
oc get addressspace <addressspace name> -o jsonpath='{.status.endpointStatuses[?(@.name=="messaging")].externalHost
----

.. Connect to the external hostname, for example:
+
[source,js]
----
const mqtt = require('mqtt')
const { MQTTPubSub } = require('@aerogear/graphql-mqtt-subscriptions')

const host = '<internal host name>'

const client = mqtt.connect({
  host: host,
  servername: host,
  username: '<MessagingUser name>',
  password: '<MessagingUser password>',
  port: 443,
  protocol: 'tls',
  rejectUnauthorized: false,
})

const pubsub = new MQTTPubSub({ client })
----

. If you use TLS, note the following additional `mqtt.connect` options:
+
* `servername` - when connecting to a message broker in OpenShift using TLS, this property must be set otherwise the connection will fail, because the messages are being routed through a proxy resulting in the client being presented with multiple certificates. By setting the `servername`, the client will use https://en.wikipedia.org/wiki/Server_Name_Indication[Server Name Indication (SNI)] to request the correct certificate as part of the TLS connection setup.
* `protocol` - must be set to `'tls'`
* `rejectUnauthorizated` - must be set to false, otherwise the connection will fail. This tells the client to ignore certificate errors. Again, this is needed because the client is presented with multiple certificates and one of the certificates is for a different hostname than the one being requested, which normally results in an error.
* `port` - must be set to 5761 for service hostname or 443 for external hostname.



== Using environment variables for configuration

{org-name} recommends that you use environment variables for connection, for example:

[source,js]
----
const mqtt = require('mqtt')
const { MQTTPubSub } = require('@aerogear/graphql-mqtt-subscriptions')

const host = process.env.MQTT_HOST || 'localhost'

const client = mqtt.connect({
  host: host,
  servername: host,
  username: process.env.MQTT_USERNAME,
  password: process.env.MQTT_PASSWORD,
  port: process.env.MQTT_PORT || 1883,
  protocol: process.env.MQTT_PROTOCOL || 'mqtt',
  rejectUnauthorized: false,
})

const pubsub = new MQTTPubSub({ client })
----

In this example, the connection options can be configured using environment variables, but sensible defaults for the `host`, `port` and `protocol` are provided for local development.

== Troubleshooting MQTT Connection Issues

=== Troubleshooting MQTT Events

The `mqtt` module emits various events during runtime.
It recommended to add listeners for these events for regular operation and for troubleshooting.

[source,js]
----
client.on('connect', () => {
  console.log('client has connected')
})

client.on('reconnect', () => {
  console.log('client has reconnected')
})

client.on('offline', () => {
  console.log('Client has gone offline')
})

client.on('error', (error) => {
  console.log(`an error has occurred ${error}`)
})
----

Read the https://www.npmjs.com/package/mqtt[`mqtt documentation`] to learn about all of the events and what causes them.

=== Troubleshooting MQTT Configuration Issues

If your application is experiencing connection errors, the most important thing to check is the configuration being passed into `mqtt.connect`. Because your application may run locally or in OpenShift, it may connect using internal or external hostnames, and it may or may not use TLS, it's very easy to accidentally provide the wrong configuration.

The Node.js `mqtt` module does not report any errors if parameters such as `hostname` or `port` are incorrect. Instead, it will silently fail and allow your application to start without messaging capabilities.

It may be necessary to handle this scenario in your application. The following workaround can be used.

[source,js]
----
const TIMEOUT = 10 // number of seconds to wait before checking if the client is connected

setTimeout(() => {
  if (!client.connected) {
    console.log(`client not connected after ${TIMEOUT} seconds`)
	// process.exit(1) if you wish
  }
}, TIMEOUT * 1000)
----

This code can be used to detect if the MQTT client hasn't connected. This can be helpful for detecting potential configuration issues and allows your application to respond to that scenario.

:leveloffset!:

:leveloffset: 1

[[sync-js-client-realtime-updates]]
= Implementing real-time updates on on the client

A core concept of the GraphQL specification is an operation type called `Subscription`, they provide a mechanism for real time updates.
For more information on GraphQL subscriptions  see the link:https://www.apollographql.com/docs/apollo-server/features/subscriptions.html[Subscriptions documentation].

To do this GraphQL Subscriptions utilise websockets to enable clients to subscribe to published changes.

The architecture of websockets is as follows:

* Client connects to websocket server.
* Upon certain events, the server can publish the results of these events to the websocket.
* Any _currently connected_ client to that websocket receives these results.
* The client can close the connection at any time and no longer receives updates.

Websockets are a perfect solution for delivering messages to currently active clients.
To receive updates the client must be currently connected to the websocket server, updates made over this websocket while the client is offline are not consumed by the client.
For this use case Push Notifications are recommended.

{sync-client} comes with subscription support out of the box including auto-reconnection upon device restart or network reconnect.
To enable subscriptions on your client set the following
paramater in the {sync-client} config object. A DataSyncConfig interface is also available from {sync-client} if you wish to use it.

== Setting up a client to use subscriptions

To set up a client to use subscriptions:

. Provide a `wsUrl` string in the config object as follows:
+
[source,javascript]
----
const config = {
    wsUrl: "ws://<your_websocket_url>"
}
----
+
where `<your_websocket_url>` is the full URL of the websocket endpoint of your GraphQL server.

. Use the object from step 1 to initialise {sync-client}:
+
[source,javascript]
----
const { createClient } = require("@aerogear/voyager-client");

const client = createClient(config)
----

== Using Subscriptions

A standard flow to utilise subscriptions is as follows:

. Make a network query to get data from the server
. Watch the cache for changes to queries
. Subscribe to changes pushed from the server
. Unsubscibe when leaving the view where there is an active subscription

In the three examples below, `subscribeToMore` ensures that any further updates received from the server force the updateQuery function to be called with `subscriptionData` from the server.

Using `subscribeToMore` ensures the cache is easily updated as all GraphQL queries are automatically notified.

For more information, see the  link:https://www.apollographql.com/docs/angular/features/subscriptions.html#subscribe-to-more[subscribeToMore documentation].

[source,javascript]
----
getTasks() {
  const tasks = client.watchQuery({
    query: GET_TASKS
  });

  tasks.subscribeToMore({
    document: TASK_ADDED_SUBSCRIPTION,
    updateQuery: (prev, { subscriptionData }) => {
    // Update logic here.
    }
  });
  return tasks;
}
----

To allow {sync-client} to automatically generate the `updateQuery` function for you, please see the <<#cache-update-helpers, Cache Update Helpers>> section.

You can then use this query in our application to subscribe to changes so that the front end is always updated when new
data is returned from the server.

[source,javascript]
----
this.tasks = [];
this.getTasks().subscribe(result => {
  this.tasks = result.data && result.data.allTasks;
})
----

Note that it is also a good idea to unsubscribe from a query upon leaving a page. This prevents possible memory leaks.
This can be done by calling unsubscribe() as shown in the following example. This code should be placed in the appropriate place.

[source, javascript]
----
this.getTasks().unsubscribe();
----

== Handling network state changes

When using subscriptions to provide your client with realtime updates it is important to monitor network state because the client will be out of sync if the server if updated when the the client is offline.

To avoid this, {sync-client} provides a `NetworkStatus` interface which can be used along with the `NetworkInfo` interface to implement custom checks of network status.

For more information about how to import and configure a custom network status checker, see xref:sync-js-client-advanced-topics[Advanced Topics].

Use the following example to re-run a query after a client returns to an online state:

[source, javascript]
----
const { CordovaNetworkStatus, NetworkInfo } = require("@aerogear/voyager-client");
const networkStatus = new CordovaNetworkStatus();

networkStatus.onStatusChangeListener({
  onStatusChange(networkInfo: NetworkInfo) {
    const online = networkInfo.online;
    if (online) {
      client.watchQuery({
        query: GET_TASKS
      });
    }
  }
});
----

:leveloffset!:

:leveloffset!:

:leveloffset: +1



[id='auth_{context}']
= Supporting authentication and authorization in your mobile app
:page-partial:
:toc:


[#sync-server-auth]
:leveloffset: 1

[[authentication-and-authorization]]
[[sync-server-auth]]
= Configuring your server for authentication and authorization using {idm-name}

Using the {keycloak-service} service and the link:https://www.npmjs.com/package/@aerogear/voyager-keycloak[@aerogear/voyager-keycloak] module, it is possible to add security to a {sync-server} application.

The `@aerogear/voyager-keycloak` module provides the following features out of the box:

* Authentication - Ensure only authenticated users can access your server endpoints, including the main GraphQL endpoint.
* Authorization - Use the `@hasRole()` directive within the GraphQL schema to implement role based access control (RBAC) on the GraphQL level.
* Integration with GraphQL context - Use the `context` object within the GraphQL resolvers to access user credentials and several helper functions.

.Prerequisites

* There is a {idm-name} service available.
* You must add a valid `keycloak.json` config file to your project.
  ** Create a client for your application in the Keycloak administration console.
  ** Click on the Installation tab.
  ** Select *Keycloak OIDC JSON* for Format option, and click *Download*.

== Protecting {sync-server} using {idm-name}

. Import the `@aerogear/voyager-keycloak` module

+
[source,javascript]
----
const { KeycloakSecurityService } = require('@aerogear/voyager-keycloak')
----

. Read the Keycloak config and pass it to initialise the `KeycloakSecurityService`.

+
[source,javascript]
----
const keycloakConfig = JSON.parse(fs.readFileSync(path.resolve(__dirname, './path/to/keycloak.json')))
const keycloakService = new KeycloakSecurityService(keycloakConfig)
----

. Use the `keycloakService` instance to protect your app:

+
[source,javascript]
----
const app = express()
keycloakService.applyAuthMiddleware(app)
----

. Configure the Voyager server so that the `keycloakService` is used as the security service:
+
[source,javascript]
----
const voyagerConfig = {
  securityService: keycloakService
}
const server = VoyagerServer(apolloConfig, voyagerConfig)
----

The link:https://github.com/aerogear/voyager-server/blob/master/examples/keycloak[Keycloak Example Server Guide] has an example server based off the instructions above and shows all of the steps needed to get it running.

== Using the hasRole directive in a schema

The Voyager Keycloak module provides the `@hasRole` directive to define role based authorisation in your schema. The `@hasRole` directive is a special annotation that can be applied to

* Fields
* Queries
* Mutations
* Subscriptions

The `@hasRole` usage is as follows:

* `@hasRole(role: String)`
  * Example - `@hasRole(role: "admin"])`
  * If the authenticated user has the role `admin` they will be authorized.
* `@hasRole(role: [String])`
  * Example - `@hasRole(role: ["admin", "editor"])`
  * If the authenticated user has at least one of the roles in the list, they will be authorized.

The default behaviour is to check client roles. For example, `@hasRole(role: "admin")` will check that user has a client role called `admin`. `@hasRole(role: "realm:admin")` will check if that user has a realm role called `admin`

The syntax for checking a realm role is `@hasRole(role: "realm:<role>")`. For example, `@hasRole(role: "realm:admin")`. Using a list of roles, it is possible to check for both client and realm roles at the same time.

.Example: Using the @hasRole Directive to Apply Role Based Authorization in a Schema

The following example demonstrates how the `@hasRole` directive can be used to define role based authorization on various parts of a GraphQL schema. This example schema represents publishing application like a news or blog website.

[source,graphql]
----
type Post {
  id: ID!
  title: String!
  author: Author!
  content: String!
  createdAt: Int!
}

type Author {
  id: ID!
  name: String!
  posts: [Post]!
  address: String! @hasRole(role: "admin")
  age: Int! @hasRole(role: "admin")
}

type Query {
  allPosts:[Post]!
  getAuthor(id: ID!):Author!
}

type Mutation {
  editPost:[Post]! @hasRole(role: ["editor", "admin"])
  deletePost(id: ID!):[Post] @hasRole(role: "admin")
}
----

There are two types:

* `Post` - This might be an article or a blog post
* `Author` - This would represent the person that authored a Post

There are two Queries:

* `allPosts` - This might return a list of posts
* `getAuthor` - This would return details about an Author

There are two Mutations:

* `editPost` - This would edit an existing post
* `deletePost` - This would delete a post.

.Role Based Authorization on Queries and Mutations

In the example schema, the `@hasRole` directive has been applied to the `editPost` and `deletePost` mutations. The same could be done on Queries.

* Only users with the roles `editor` and/or `admin` are allowed to perform the `editPost` mutation.
* Only users with the role `admin` are allowed to perform the `deletePost` mutation.

This example shows how the `@hasRole` directive can be used on various queries and mutations.

.Role Based Authorization on Fields

In the example schema, the `Author` type has the fields `address` and `age` which both have `hasRole(role: "admin")` applied.

This means that users without the role `admin` are not authorized to request these fields in any query or mutation.

For example, non admin users are allowed to run the `getAuthor` query, but they cannot request back the `address` or `age` fields.

:leveloffset!:

:leveloffset: 1

[#sync-server-auth-websockets]
[id="authentication-and-authorization-websockets-{context}"]
= Authentication Over Websockets using {idm-name}

Prerequisites:

* xref:sync-server-auth[Configure Data Sync Server for Authentication and Authorization]
* xref:ds-realtime.adoc#realtime-updates-{context}[Configuring Your Server for real-time updates]

This section describes how to implement Authentication and Authorization over websockets with {idm-name}. For more generic documentation on Authentication over Websockets, read Apollo's https://www.apollographql.com/docs/apollo-server/features/subscriptions/#authentication-over-websocket[Authentication Over Websocket] document.

The Voyager Client supports adding token information to `connectionParams` that will be sent with the first WebSocket message. In the server, this token is used to authenticate the connection and to allow the subscription to proceeed. Read the section on xref:sync-js-client-auth[{idm-name} Authentication in Voyager Client] to ensure the {idm-name} token is being sent to the server.

In the server, `createSubscriptionServer` accepts a `SecurityService` instance in addition to the regular options that can be passed to a standard `SubscriptionServer`. The `KeycloakSecurityService` from `@aerogear/voyager-keycloak` is used to validate the {idm-name} token passed by the client in the initial WebSocket message.

[source,js]
----
const { createSubscriptionServer } = require('@aerogear/voyager-subscriptions')
const { KeycloakSecurityService } = require('@aerogear/voyager-keycloak')
const keycloakConfig = require('./keycloak.json') // typical Keycloak OIDC installation

const apolloServer = VoyagerServer({
  typeDefs,
  resolvers
})

securityService = new KeycloakSecurityService(keycloakConfig)

const app = express()

keycloakService.applyAuthMiddleware(app)
apolloServer.applyMiddleware({ app })

const server = app.listen({ port }, () =>
  console.log(`🚀 Server ready at http://localhost:${port}${apolloServer.graphqlPath}`)

  createSubscriptionServer({ schema: apolloServer.schema }, {
    securityService,
    server,
    path: '/graphql'
  })
)
----

The example shows how the {idm-name} `securityService` is created and how it is passed into `createSubscriptionServer`. This enables {idm-name} authentication on all subscriptions.

== {idm-name} Authorization in Subscriptions

The {idm-name} `securityService` will validate and parse the token sent by the client into a https://github.com/keycloak/keycloak-nodejs-connect/blob/master/middleware/auth-utils/token.js[Token Object]. This token is available in Subscription resolvers with `context.auth` and can be used to implement finer grained role based access control.

[source,js]
----
const resolvers = {
  Subscription: {
    taskAdded: {
      subscribe: (obj, args, context, info) => {
        const role = 'admin'
        if (!context.auth.hasRole(role)) {
          return new Error(`Access Denied - missing role ${role}`)
        }
        return pubSub.asyncIterator(TASK_ADDED)
      }
    },
}
----

The above example shows role based access control inside a subscription resolver. `context.auth` is a full https://github.com/keycloak/keycloak-nodejs-connect/blob/master/middleware/auth-utils/token.js[Keycloak Token Object] which means methods like `hasRealmRole` and `hasApplicationRole` are available.

The user details can be accessed through `context.auth.content`. Here is an example.

----
{
  "jti": "dc1d6286-c572-43c1-99c7-4f36982b0e56",
  "exp": 1561495720,
  "nbf": 0,
  "iat": 1561461830,
  "iss": "http://localhost:8080/auth/realms/voyager-testing",
  "aud": "voyager-testing-public",
  "sub": "57e1dcda-990f-4cc2-8542-0d1f9aae302b",
  "typ": "Bearer",
  "azp": "voyager-testing-public",
  "nonce": "552c3cba-a6c2-490a-9914-28784ba0e4bc",
  "auth_time": 1561459720,
  "session_state": "ed35e1b4-b43c-438f-b1a3-18b1be8c6307",
  "acr": "0",
  "allowed-origins": [
    "*"
  ],
  "realm_access": {
    "roles": [
      "developer",
      "uma_authorization"
    ]
  },
  "resource_access": {
    "voyager-testing-public": {
      "roles": [
        "developer"
      ]
    },
    "account": {
      "roles": [
        "manage-account",
        "manage-account-links",
        "view-profile"
      ]
    }
  },
  "preferred_username": "developer"
}
----

Having access to the user details (e.g. `context.auth.content.sub` is the authenticated user's ID) means it is possible to implement https://www.apollographql.com/docs/apollo-server/features/subscriptions/#subscription-filters[Subscription Filters] and to subscribe to more fine grained pubsub topics based off the user details.

:leveloffset!:

[#sync-js-client-auth]
:leveloffset: 1

= Implementing authentication and authorization on your client

With {sync-client}, user information can be passed to a {sync-service} server application in two ways: headers or tokens.

Headers are used to authentication HTTP requests to the server, which are used for queries and mutations.

Tokens are used to authenticate WebSocket connections, which are used for subscriptions.

Both of them can be set via the `authContextProvider` configuration option. Here is an example

[source, javascript]
----

//get the token value from somewhere, for example the authentication service
const token = "REPLACE_WITH_REAL_TOKEN";

const config = {
  ...
  authContextProvider: function() {
    return {
      header: {
        "Authorization": `Bearer ${token}`
      },
      token: token
    }
  },
  ...
};

//create a new client
----

For information about how to perform authentication and authorization on the server, see the <<#sync-server-auth, Server Authentication and Authorization Guide>>.

:leveloffset!:

:leveloffset!:

// include::{upstream-location}/ds-metric.adoc[leveloffset=+1, tags=!excludeDownstream]

:leveloffset: +1



[id='files{context}']
= Allowing users upload files from your mobile app
:page-partial:
:toc:

:leveloffset: 1

= Enabling file uploads on the server

{sync-server} provides support for uploading binary data along with the GraphQL queries.
The implementation relies on upstream `Apollo Server` capabilities.

The upload functionality uses the GraphQL multipart form requests specification.
File upload needs to be implemented on both server and client:

1. On the client HTML FileList objects are mapped into a mutation and sent to the server in a multipart request.

2. On the server: The multipart request is handled. The server processes it and provides an upload argument to a resolver.
In the resolver function, the upload promise resolves an object.

NOTE: File upload is based on link:https://github.com/jaydenseric/graphql-multipart-request-spec[graphql-multipart-request-spec].

.Procedure

To enable file uploads, create a schema and use the `Upload` scalar.
For example:

[source, javascript]
----
const { ApolloServer, gql } = require('apollo-server');

const typeDefs = gql`
  type File {
    filename: String!
    mimetype: String!
    encoding: String!
  }
  type Query {
    uploads: [File]
  }
  type Mutation {
    singleUpload(file: Upload!): File!
  }
`;
----

The following schema enables file uploads. The `Upload` scalar will be injected as one of the arguments in the resolvers.
The `Upload` scalar contains all file metadata and a link:https://nodejs.org/api/stream.html#stream_readable_streams[Readable Stream] that can be used to save the file to a specific location.

[source, javascript]
----

    async singleUpload(parent, { file }) {
      const { stream, filename, mimetype, encoding } = await file;
      // Save file and return required metadata
    }
----

See link:https://blog.apollographql.com/file-uploads-with-apollo-server-2-0-5db2f3f60675[Official Apollo blog post] for more information.

:leveloffset!:

:leveloffset: 1

= Implementing file upload on the client

{sync-client} provides support for uploading binary data along with the GraphQL queries.
The binary upload implementation uses the `apollo-upload-client` package built by the Apollo community.

== Introduction

The upload functionality uses the GraphQL multipart form requests specification.
The File upload needs to be implemented on both server and client:

1. On the client HTML FileList objects are mapped into a mutation and sent to the server in a multipart request.

2. On the server: The multipart request is handled. The server processes it and provides an upload argument to a resolver.
In the resolver function, the upload promise resolves an object.

NOTE: File upload is based on link:https://github.com/jaydenseric/graphql-multipart-request-spec[graphql-multipart-request-spec].

== Enabling File Upload

File upload feature needs to be enabled by passing `fileUpload` flag to config object:

[source, javascript]
----

const config = {
  ...
  fileUpload: true
  ...
};

//create a new client
----

= Uploading Files from GraphQL

File upload capability adds a new GraphQL scalar `Upload` that can be used for mutations that operate on binary data.
The `Upload` scalar maps html `FileList` HTML5 object in GraphQL schemas.
The first step required to work with binary uploads is to write mutation that will contain `Upload` scalar.
The following example demonstrates how to upload a profile picture:

[source, javascript]
----
import gql from 'graphql-tag'
import { Mutation } from 'react-apollo'

export const UPLOAD_PROFILE = gql`
mutation changeProfilePicture($file: Upload!) {
  changeProfilePicture(file: $file) {
    filename
    mimetype
    encoding
  }
}
`;
----


== Executing mutations

The `Upload` scalar will be mapped  to object returned from HTML file input.

The following example shows file upload in a React application.


[source, javascript]
----

const uploadOneFile = () => {
  return (
    <Mutation mutation={UPLOAD_PROFILE}>
      {uploadFile => (
        <input
        type="file"
        required
        onChange={({ target: { validity, files: [file] } }) =>
          validity.valid && uploadFile({ variables: { file } });
        }
       />
      )}
    </Mutation>
  );
};
----

:leveloffset!:

:leveloffset!:

:leveloffset: +1



[id='audit_{context}']
= Enabling audit logs and viewing reports
:page-partial:
:toc:

:leveloffset: 1

[#sync-server-audit-logs]
= Enabling audit logs on the server

Audit logging is a mechanism to track all of the actions that occur inside your application. Audit Logging in {sync-server} provides two main benefits.

1. It is possible to build a detailed audit trail of every action that has occured in the application. This can also include information about the user that performed the action, and the mobile device they were using.
2. The data from the audit logs can be aggregated and visualised to provide more insight into how your application is used.

// This overview is what is called *metrics* in the rest of this document.

== Audit Logging Architecture

Audit logging can be enabled in {sync-server} using the link:https://www.npmjs.com/package/@aerogear/voyager-audit[@aerogear/voyager-audit] module. When enabled, all actions such as GraphQL mutations, queries and subscriptions are logged in great detail to `stdout` in JSON format.

An audit log example message is shown below.

[source,json]
----
{
  "level": 30,
  "time": 1545385687476,
  "pid": 11889,
  "hostname": "localhost.localdomain",
  "tag": "AUDIT",
  "logType" "RESOLVER_COMPLETION",
  "msg": "",
  "operationType": "query",
  "fieldName": "hello",
  "parentTypeName": "Query",
  "path": "hello",
  "success": true,
  "arguments": {},
  "clientInfo": {
    "clientId": "848d2a10-0505-11e9-888f-8d166149101a",
    "timestamp": 1545385686843,
    "data": {
      "app": {
        "appId": "org.aerogear.sync.example",
        "appVersion": "0.0.1",
        "sdkVersion": "0.0.1",
        "framework": "cordova"
      },
      "device": {
        "platform": "android",
        "platformVersion": "9",
        "device": "General Mobile GM8 Pro"
      }
    }
  },
  "userInfo": {
    "jti": "6ae0966a-9d61-430b-8167-d2b3c0b42709",
    "exp": 1545761725,
    "nbf": 0,
    "iat": 1545725725,
    "iss": "http://localhost:8080/auth/realms/voyager-testing",
    "aud": "voyager-testing",
    "sub": "ea2312e9-1aae-4b67-8674-a3aacf20a71d",
    "typ": "Bearer",
    "azp": "voyager-testing",
    "auth_time": 1545725725,
    "session_state": "1ba4d429-8010-4f38-8002-9cc72550850d",
    "acr": "1",
    "allowed-origins": [
      "*"
    ],
    "realm_access": {
      "roles": [
        "admin",
        "uma_authorization"
      ]
    },
    "resource_access": {
      "voyager-testing": {
        "roles": [
          "admin"
        ]
      },
      "account": {
        "roles": [
          "manage-account",
          "manage-account-links",
          "view-profile"
        ]
      }
    },
    "name": "Ali Ok",
    "preferred_username": "developer",
    "given_name": "Ali",
    "family_name": "Ok",
    "email": "aliok@example.com"
  },
  "v": 1
}
----

The `clientInfo` property of the audit log message is available only if the client is sending the client information to {sync-server}. That has to be enabled separately in the client. Additionally, data in that property can only be collected if the app is a Cordova app or a native app. Simple web clients cannot get the device, client nor app details and cannot send this information.

The `userInfo` property is available only if {sync-server} is protected by an identity manager, such as {idm-name}, and if the user is authenticated. See  see xref:sync-server-auth[].

== Enabling Audit Logging in Voyager Server

Audit logging is enabled in {sync-server} using the link:https://www.npmjs.com/package/@aerogear/voyager-audit[@aerogear/voyager-audit]

. Import the `@aerogear/voyager-audit` module
+
[source,javascript]
----
const auditLogger = require('@aerogear/voyager-audit')
----

. Inject the auditLogger module into the {sync-server}. This enables audit logging within your application.
+
[source,javascript]
----
const voyagerConfig = {
  auditLogger
}
const server = VoyagerServer(apolloConfig, voyagerConfig)
----

The link:https://github.com/aerogear/voyager-server/tree/master/examples/audit_logging[Audit Logging Example Server Guide] has an example server based off the instructions above and shows all of the steps needed to get it running.

Alternatively, if the default audit logger does not match your requirements, you can create an audit logger that implements the `AuditLogger` interface as defined below.

.Definition of the `AuditLogger` interface

[source,typescript]
----
export interface AuditLogger {
  logResolverCompletion(msg: string, success: boolean, obj: any, args: any, context: any, info: GraphQLResolveInfo): void
  logConflict (msg: string, serverData: any, clientData: any, obj: any, args: any, context: any, info: GraphQLResolveInfo): void
  auditLog(msg: string, obj: any, args: any, context: any, info: GraphQLResolveInfo): void
}
----

The following example implements an `AuditLogger` and injects it into the {sync-server}.
The example redacts the arguments using a `myCustomRedactionFunction` function.

[source,typescript]
----

const { buildPath } = require('@aerogear/voyager-tools')
// ...

const auditLogger = {
  auditLog: function(msg, obj, args, context, info){
    console.log(JSON.stringify(
      {
        audit: {
          tag: 'AUDIT',
          logType: logType,
          msg: msg || '',
          requestId: context && context.request ? context.request.id : '',
          operationType: info.operation.operation,
          fieldName: info.fieldName,
          parentTypeName: info.parentType.name,
          path: buildPath(info.path),
          parent: obj,
          arguments: myCustomRedactionFunction(args),
          clientInfo: context && context.request && context.request.body && context.request.body.extensions && context.request.body.extensions.metrics || undefined,
          authenticated: !!(context && context.auth && context.auth.isAuthenticated()),
          userInfo: (context && context.auth && context.auth.accessToken) ? context.auth.accessToken.content : undefined
        }
      }
    ));
  },

  logResolverCompletion: function(msg, success, obj, args, context, info){
    console.log(JSON.stringify(
      {
        audit: {
          tag: 'AUDIT',
          logType: 'RESOLVER_COMPLETION',
          msg: msg || '',
          requestId: context && context.request ? context.request.id : '',
          operationType: info.operation.operation,
          fieldName: info.fieldName,
          parentTypeName: info.parentType.name,
          path: buildPath(info.path),
          success,
          parent: obj,
          arguments: myCustomRedactionFunction(args),
          clientInfo: context && context.request && context.request.body && context.request.body.extensions && context.request.body.extensions.metrics || undefined,
          authenticated: !!(context && context.auth && context.auth.isAuthenticated()),
          userInfo: (context && context.auth && context.auth.accessToken) ? context.auth.accessToken.content : undefined
        }
      }
    ));
  },

  logConflict: function (msg, serverData, clientData, obj, args, context, info) {
    console.log(JSON.stringify(
      {
        audit: {
          tag: 'AUDIT',
          logType: LOG_TYPE_CONFLICT,
          msg: msg || '',
          requestId: context && context.request ? context.request.id : '',
          operationType: info.operation.operation,
          fieldName: info.fieldName,
          parentTypeName: info.parentType.name,
          path: buildPath(info.path),
          parent: obj,
          arguments: myCustomRedactionFunction(args),
          clientInfo: context && context.request && context.request.body && context.request.body.extensions && context.request.body.extensions.metrics || undefined,
          authenticated: !!(context && context.auth && context.auth.isAuthenticated()),
          userInfo: (context && context.auth && context.auth.accessToken) ? context.auth.accessToken.content : undefined,
          conflict: true,
          conflictData: {
            message: msg,
            myCustomRedactionFunction(serverData),
            myCustomRedactionFunction(clientData),
          }
        }
      }
    ));
  }
}

// ...

const voyagerConfig = {
  auditLogger
}
const server = VoyagerServer(apolloConfig, voyagerConfig)

----

== Sending Device Information in {sync-client}

See the <<#sync-js-client-audit-logs, {sync-client} Audit Logs>> section for more information.


== Exploring Audit Logs

{sync-server} simply prints audit logs to `stdout` and it is the responsibility of another component to pick up these logs and provide
functionality to the user to make use of the logs.

The *EFK stack* (ElasticSearch, Fluentd and Kibana) on OpenShift is the recommended solution in this guide. We provide Kibana dashboards with a number of useful visualisations and insights into {sync-server}.

All application logs printed to `stdout` are sent to ElasticSearch by Fluentd. However, the audit log messages printed by `@aerogear/voyager-audit` are printed in a format that is used by the Kibana dashboards.

== Configuring OpenShift Logging

OpenShift logging can be enabled as described in link:https://docs.okd.io/3.11/install_config/aggregate_logging.html[OpenShift documentation].

Once enabled, OpenShift logging will create a Fluentd instance per cluster node that reads the `stdout` and `stderr` of the pods in that node
and pushes the readings to the centralized ElasticSearch instance. Documents created in ElasticSearch instance can be then explored and
visualized by the Kibana instance, which is also installed by OpenShift logging.

OpenShift logging creates an index per namespace and that index is only available to users who have access to that namespace.
It also creates the index patterns in Kibana in the same way.

By default, OpenShift also provides a https://www.elastic.co/guide/en/elasticsearch/client/curator/current/about.html[curator] which deletes the old
log messages from ElasticSearch to reduce storage needs and improve performance. This has an impact on audit trails and also metrics.

For long term audit trails, curator can be configured to delete messages older than your choice. If this is not sufficient,
Fluentd can be configured to write log messages to a separate storage, such as link:https://docs.fluentd.org/v0.12/articles/out_s3[S3].

In terms of metrics, curator's deletion age config should not be set shorter than the desired time range that you would like
to see the metrics for.


== Importing Kibana Saved Objects

Kibana is a visualization tool that has a great integration with ElasticSearch.

A template for Kibana saved objects is available. When the saved objects are imported, a number of saved searches, visualizations and a
dashboard are created in Kibana. These then can be used to have an overview of the Voyager application.

See the screenshot of the provided dashboard below.

image::kibana-dashboard-screenshot.png[]

OpenShift logging creates ElasticSearch indices per namespace and the index names have the format `project.<project-name>.<project-uid>`.
For example `project.myproject.49f9a0b6-09b5-11e9-9597-069f7827c758`.

It also creates a Kibana index pattern for that index using the pattern `project.<project-name>.<project-uid>.{asterisk}`.

In order to make sure the Kibana saved objects use the correct index pattern, project UID should be fetched and
fed to the Kibana import template.

[source,bash]
----
PROJECT_NAME=<your_project_name>
# login with your user that has access to your project
oc login
# get project UUID, which is used to build the index name
PROJECT_UUID=`oc get project $PROJECT_NAME -o go-template='{{.metadata.uid}}'`

# replace the placeholders in the template
sed \
    -e "s/<PROJECT_NAME>/${PROJECT_NAME}/g" \
    -e "s/<PROJECT_UUID>/${PROJECT_UUID}/g" \
 kibanaImportTemplate.json > kibanaImport.json
----

A template, `kibanaImportTemplate.json` is available from the link:https://raw.githubusercontent.com/aerogear/voyager-server/master/doc/guides/kibanaImportTemplate.json[Voyager GitHub repo].

Once the `kibanaImport.json` file is generated, import it into Kibana:

* Open Kibana using `https://kibana.<domain>.com`. Replace `<domain>` with the name of the cluster's main domain.
* Click *Management* in the left
* Click *Saved Objects*
* Click *Import* and select `kibanaImport.json`

Imported saved objects include the project name or the UID in their names, so that saved objects in differnt namespaces do not affect each other.


[NOTE]
====
No index pattern is created in Kibana if there are no logs generated by an application.

Also, if the fields referenced in the prepared Kibana saved objects do not exist, errors such as the following can be seen:

----
Error: Importing AeroGear Data Sync - top level execution per platform - aaa (top_level_execution_per_platform_49f9a0b6-09b5-11e9-9597-069f7827c758) failed: Could not locate that index-pattern-field (id: audit.clientInfo.data.device.platform.raw)
Error: Could not locate that index-pattern-field (id: audit.clientInfo.data.device.platform.raw)
----

Because of these conditions, Kibana saved objects have to be imported after there are some audit logs already in ElasticSearch.
At the moment, no mechanisms are provided to overcome this problem.
====

= Viewing the Dashboard and Audit Logs

When the Kibana saved objects are imported, a dashboard is available with several visualizations that can be used as an
overview of the Voyager application status.

At the bottom of the dashboard, audit log messages can be explored directly.

For more information on how to use Kibana, see the link:https://www.elastic.co/products/kibana[Kibana documentation].

:leveloffset!:

[#sync-js-client-audit-logs]
:leveloffset: 1

= Enabling audit logs on the client

As described in the <<#sync-server-audit-logs, Server Audit Logs>> section, device information can be logged as part of an audit log message. To enable it:

. The Cordova plugin `cordova-plugin-aerogear-metrics` has to be installed so that the device, client and app information can be collected.

+
[source,bash]
----
cordova plugin add cordova-plugin-aerogear-metrics
----

. Set `auditLogging` to true when creating a client instance.

+
[source,javascript]
----
import { createClient } from '@aerogear/voyager-client';

const config = {
  ...
  auditLogging: true,
  ...
}

return await createClient(config);
----

:leveloffset!:

:leveloffset!:

:leveloffset: +1



:leveloffset: 1

[[sync-js-client-advanced-topics]]
= Advanced Topics
:toc:

== Logging Debug Messages

The {sync-client} uses the link:https://www.npmjs.com/package/debug[debug module] to log debug messages.

To enable debug logs, run the following code in a browser's console:

[source, javascript]
----
localStorage.debug = 'AeroGearSync:*'
----

Certain features can be enabled separately:

[source, javascript]
----
localStorage.debug = 'AeroGearSync:offlineMutates*'
----

== Optimistic UI

By default mutations are not applied to the UI until responses are received from the server. To provide better user experience, an application may want to update the UI immediately. link:https://www.apollographql.com/docs/react/api/react-apollo.html#graphql-mutation-options-optimisticResponse[Optimistic response] is an easy way to achieve this goal, and {sync-client} provides a helper method to work with optimistic responses:

[source, javascript]
----
 import { createOptimisticResponse } from "@aerogear/voyager-client";

 client.mutate({
   mutation: ADD_TASK,
   variables: item,
   optimisticResponse: createOptimisticResponse("createTask", "Task", item);
 });
----

To detect if the provided data is an optimistic response, the `optimisticResponse` flag can be used.

//Is this id mapping completely transparent to developers? If not, what action do they need to take? If it is, then we should probably remove this section.
// === Mapping Client and Server ID for Optimistic Responses

// When using `OptimisticReponse` helper from SDK specific mutations that create new element response is going to have client side generated id. Subsequent edits for this objects will also refer to this id. When becoming online, all offline changes are going to be performed in specific order updating client side id with id returned from server for subsequent edits.

The `OptimisticResponse` feature and the <<#sync-client-offline-queue-listener, offlineQueueListener>> can be used together to deliver great offline experience for an application.



:leveloffset: 1

:leveloffset!:

:leveloffset: +1



[id='openshift_{context}']
= Running a {sync-service} app on OpenShift
:page-partial:
:toc:
[#sync-server-getting-started-openshift]
:leveloffset: 1

.Prerequisites
* You have Docker installed on your local machine.
* You have access to an OpenShift cluster with the Service Catalog.
* You have completed the server getting started guide.

= Overview

To connect your Data Sync server and client to other services, you need to run your application in OpenShift.  Data Sync provides a service catalog item to help with this.

Data Sync requires your server application to be packaged as a Docker formatted container and published to a public respository such as link:https://hub.docker.com/[Docker hub].

[#building-and-publishing-the-container]
= Building and publishing the {sync-service} server container

To build a server into a container, create a `Dockerfile` in the project's directory.  This container will need to include your server source code, its dependencies, and be configured to execute your server.

As an example:

[source,dockerfile]
----
FROM node:8
WORKDIR /usr/src/app
# copy Node.js specific files
COPY package*.json ./
# copy application source file to the workdir
COPY index.js .
RUN npm install
# TCP port that application is listening on
EXPOSE 4000
CMD [ "node", "index.js" ]
----

Build the Docker container and tag it:
[source,bash]
----
$ docker build . --tag  <your-repo>/<container-name>
----

Push your container to Dockerhub's repository:
[source,bash]
----
$ docker push <your-repo>/<container-name>
----

:leveloffset: 1

[#sync-server-provisioning-data-sync-templates]
:leveloffset: 1


= Provisioning the {sync-service} server applications using templates

{sync-service} offers following OpenShift templates
that will help developers with provisioning their DataSync applications to OpenShift platform.

Templates:

- DataSync App

The DataSync App template allows developers to deploy the Node.js DataSync App on Openshift using source code only.
_Node s2i_  is used to build the Data Sync App image.
NOTE: You must set the `NODE_ENV` environment variable to `development` and redeploy the pod to ensure access to the GraphQL playground.

The DataSync App can connect with other services running on OpenShift and can also connect to external data sources.

- DataSync Showcase

Showcase application will deploy fully functional server with example Task implementation.
Server side requires client application available on github link:https://github.com/aerogear/ionic-showcase[aerogear/ionic-showcase]

> Note: Showcase server template can be used only for demo purposes and it should not be used in production.


When running on Red Hat Managed Integration templates will be available in *Mobile* > *App*  category in OpenShift catalog



:leveloffset: 1

[#sync-server-getting-started-mdc-client]
:leveloffset: 1

= Connecting a Client

.Prerequisites
* You have access to an OpenShift cluster with the Service Catalog.
* You have completed the OpenShift getting started guide.
* You have created a mobile client and bound your data sync server.
* You have completed the client getting started guide.

Once a service is bound to a mobile client, MDC will provide a mobile-services.json file that is used by the AeroGear client libraries to automatically configure the Data Sync client.  It is very important that you use your version of this file and not the one used in this example as system specific values will be different.

== Updating the Hello World Sync Client

The Hello World client application we wrote uses a hard coded server url.  We need to remove this url and instead pass the mobile-services config to the client.  We will also use the AeroGear core library to parse this file and pass that configuration to the Data Sync library.

.Configure the core library with mobile-services.json
[source, javascript]
----
const { init }  = require("@aerogear/app");

const core = init({
  "version": 1,
  "namespace": "myproject",
  "clientId": "getting-started",
  "services": [
    {
      "id": "0637bfd3-33aa-11e9-968e-52540014a8c2",
      "name": "sync-app-getting-started-getting-started",
      "type": "sync-app",
      "url": "https://sync-app-getting-started-myproject.192.168.42.138.nip.io/graphql",
      "config": {
        "websocketUrl": "wss://sync-app-getting-started-myproject.192.168.42.138.nip.io/graphql"
      }
    }
  ]
});
----

Once you have initialized the core, we can use it to configure the Data Sync client by setting the `openShiftConfig` property when we call `createClient`.

.Updated data sync client
[source,javascript]
----
let client = await createClient({
    openShiftConfig:core.config
  });
----

And now, as before, we can use the client to make queries.  A full example may look like the following code

.Updated hello world index.js
[source,javascript]
----
import gql from 'graphql-tag';
const { init }  = require("@aerogear/app");
import { createClient } from '@aerogear/voyager-client';

const core = init({
  "version": 1,
  "namespace": "myproject",
  "clientId": "getting-started",
  "services": [
    {
      "id": "0637bfd3-33aa-11e9-968e-52540014a8c2",
      "name": "sync-app-getting-started-getting-started",
      "type": "sync-app",
      "url": "https://sync-app-getting-started-myproject.192.168.42.138.nip.io/graphql",
      "config": {
        "websocketUrl": "wss://sync-app-getting-started-myproject.192.168.42.138.nip.io/graphql"
      }
    }
  ]
});

async function helloWorld() {
  let client = await createClient({
    openShiftConfig:core.config
  });
  client.query({
      fetchPolicy: 'network-only',
      query: gql`{hello}`
    })
    .then( ({data}) => {
      console.log(data.hello)
    });
}

helloWorld();

----

:leveloffset: 1

[#sync-server-binding]
:leveloffset: 1


= Binding a {mobile-client} with the {sync-service} server application service

To use {product-name}, you must represent your mobile app in *Mobile Developer Console*, and that app must be associated with the mobile service.
This association is called *binding* and it is necessary for your mobile app to use that service.

To bind a {mobile-client} with a mobile service:

.Procedure

. Launch {mobile-developer-console}

. Click on the {mobile-client} on the Overview screen

. Navigate to *Mobile Services* tab.
+
image::mobile-clients-services-all-unbound.png[]
+
NOTE: It is possible to bind a {mobile-client} with a mobile service in the OpenShift console, however such bindings are not valid for the purposes of this procedure.

. Press *Bind to App* in the {service-name}
. Fill out the binding parameters required by the {service-name} Service.

:leveloffset: 1

[#sync-server-binding-keycloak]
:leveloffset: 1

= Binding {sync-service} to {keycloak-service}

In this section, we will show you how to protect your {sync-service} application using the {keycloak-service} service.

.Prerequisites

- There is a {keycloak-service} service available.
- You have provisioned a {sync-service} application using our playbook.
- oc tools must be installed

Any application that connects to {keycloak-service} must consume a `keycloak.json` file. This section demonstrates how to add a `keycloak.json` file to your {sync-service} application deployment. It is still your application's responsibility to consume this file. We have provided an link:https://github.com/aerogear/voyager-server/tree/master/examples/keycloak[example project].

.Procedure

. Create a client for your application in the {keycloak-service} Administration Console
. Click on the `Installation` tab, select `Keycloak OIDC JSON` for `Format` Option, and then click on `Download`. Save the downloaded `keycloak.json`.

. Create a {keycloak-service} secret:
+
[source, bash]
----
oc create secret generic sync-keycloak-doc \
  --from-file=keycloak=./keycloak.json
----
+
The command creates a secret named `sync-keycloak-doc`.
+
The secret will contain one key, `keycloak`, with the value being the text of the `keycloak.json` file.
+
You can view the secret with either `oc get secret sync-keycloak-doc` or the OpenShift web console.

. Create a patch for your deployment configuration
+
This step requires patching the {sync-service} application's deployment config to create and mount a volume with the {keycloak-service} secret we just created. Replace `$YOUR_DEPLOYMENT_CONFIG_NAME` in the following yaml section with the deployment config name of your {sync-service} application and save this file as `secret.yaml`.
+
[source, yaml]
----
spec:
  template:
    spec:
      containers:
        - env:
          - name: KEYCLOAK_CONFIG
            value: /config/keycloak.json
          name: $YOUR_DEPLOYMENT_CONFIG_NAME
          volumeMounts:
            - name: secret-volume
              mountPath: /config
              readOnly: true
      volumes:
          - name: secret-volume
            secret:
              items:
                - key: keycloak
                  path: keycloak.json
              secretName: sync-keycloak-doc
----

. Apply the patch.
+
After replacing `$YOUR_DEPLOYMENT_CONFIG_NAME` with the deployment config name, run the following command to patch the deployment configuration and trigger your application to redeploy.
+
[source, bash]
----
oc patch deploymentconfig $YOUR_DEPLOYMENT_CONFIG_NAME -p "$(cat secret.yaml)"
----
+
Once your application has redeployed, it should be able to read the keycloak.json file pointed to by the KEYCLOAK_CONFIG environment variable.

:leveloffset: 1

:leveloffset!:
